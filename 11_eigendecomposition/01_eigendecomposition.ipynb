{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvectors and Eigenvalues\n",
    "\n",
    "Before we start talking about eigendecomposition, we need to think about whateigenvectors and eigenvalues are.\n",
    "\n",
    "Recall that a matrix A can be expressed as some rotating and stretching of a field in some dimension of space. \n",
    "\n",
    "Eigenvectors are vectors in this space that do not change their direction after applying the transformation A, but they may be scales by some value (eigenvalue).\n",
    "\n",
    "$$ Av = \\lambda v$$\n",
    "\n",
    "If we think of this in terms of data, the eivenvectors are the points that have the most variance, this concept is pretty important in statistics.\n",
    "\n",
    "## How to find Eigenvals and Eigenvecs\n",
    "\n",
    "### Eigenvalues\n",
    "\n",
    "If the above equation is true, then so must the following equations:\n",
    "\n",
    "$$Av - \\lambda v = 0$$\n",
    "$$(A - \\lambda I) v = 0$$\n",
    "\n",
    "Note that when we shift A in this way then it becomes a singular matrix with a determinant of 0.\n",
    "\n",
    "$$| A - \\lambda I | = 0$$\n",
    "\n",
    "The determinant of this matrix will be an $n^{th}$power polynomial with n solutions, where n is the shape of the square matrix A. (Eigendecomposition does not work for rectangular matrices.)\n",
    "\n",
    "I won't focus on how exactly we'll do this because this is something I have a computer to do. See notes for more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3, 1],\n",
       "        [4, 6]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "from scipy import linalg as la\n",
    "\n",
    "A = np.matrix(\"3 1;4 6\")\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The polynomial for 2x2 matrices is y^2 - tr(A)y + det\n",
    "trace = int(A.trace())\n",
    "det = int(np.linalg.det(A))\n",
    "\n",
    "trace, det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = sp.Symbol('x')\n",
    "\n",
    "l_1, l_2 = sp.solvers.solve(l**2 - trace*l + det, l)\n",
    "\n",
    "l_1, l_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the eigenvalues are 2 and 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors\n",
    "\n",
    "Once we have the eigenvalues from the matrix, we can think about how we get the eigenvectors.\n",
    "\n",
    "We simply substitute lambda for the eigenvalues we attained and find a vector that lies in the nullspace of this matrix. These vectors are the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0.],\n",
       "       [0., 7.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create eigenvalue matrix (variance matrix)\n",
    "L = np.zeros(shape=(2, 2))\n",
    "L[0, 0] = l_1\n",
    "L[1, 1] = l_2\n",
    "\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678],\n",
       "       [-0.70710678]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shift A by l\n",
    "A_shift = A - np.identity(2)*l_1\n",
    "\n",
    "# Find basis for nullspace\n",
    "la.null_space(np.matrix(\"1 1;4 4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that this is a [1, -1] vector that has been scaled to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24253563],\n",
       "       [0.9701425 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shift A by l\n",
    "A_shift = A - np.identity(2)*l_2\n",
    "\n",
    "# Find basis for nullspace\n",
    "la.null_space(np.matrix(\"-4 1; 4 -1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well, we have found the other eigenvector for this matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "Notice that we can combine our eigenvectors to make an eigenvector matrix V and an eigenvalue matrix D with eigenvalues on the diagonal.\n",
    "\n",
    "Thus we can turn our previous equation into a matrix equation:\n",
    "\n",
    "$$ AV = V \\Lambda$$\n",
    "\n",
    "or\n",
    "\n",
    "$$A = V \\Lambda V^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, L = np.linalg.eig(A)\n",
    "\n",
    "Dm = np.zeros(shape=(2, 2))\n",
    "Dm[0, 0] = D[0]\n",
    "Dm[1, 1] = D[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3., 1.],\n",
       "        [4., 6.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.dot(Dm).dot(np.linalg.inv(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how we can combine these matrices to create the full matrix again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice property of the eigendecomposition is that it becomes very easy to calculate repeated applications  of the same matrix\n",
    "\n",
    "$A^TA = V \\Lambda V^{-1} V \\lambda V^{-1}$\n",
    "\n",
    "Thus, repeated iterations give you the following...\n",
    "\n",
    "$A^n = V \\Lambda ^n V^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this decomposition is valid for singular matrices as well, it's just that some eigenvalues will be 0. (Which makes sense since it denotes an eigenvector being squashed to 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition of Symmetric Matrices\n",
    "\n",
    "It's worth noting that if A is a symmetric matrix then then V will be orthogonal. This is an important point for SVD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
