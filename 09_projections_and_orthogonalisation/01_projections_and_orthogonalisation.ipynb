{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projections and Orthogonalisation\n",
    "\n",
    "## Projections.\n",
    "\n",
    "Say that we have a 2 dimensional vector and a point that does not lie in the column space of that vector. How do we best estimate this point such that the point lies in the column space of the vector?\n",
    "\n",
    "The first thing we can note is that the dot product between the vector and the vector created between the point and the original vector will be 0.\n",
    "\n",
    "$$a^T(b-\\beta a) = 0$$\n",
    "\n",
    "Where a is the original vector, b is the point, and beta is the scaling factor by which we estimate the best value of a. We then solve for beta to find this best scaling coefficient.\n",
    "\n",
    "$$ \\beta = \\frac{a^Tb}{a^Ta}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2]), array([2, 2]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Define vector a and point b\n",
    "a = np.array([1, 2])\n",
    "b = np.array([2, 2])\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no way we can multiply a by a scalar to get b, hance it is not in the column space of a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find best a\n",
    "beta = a.dot(b)/a.dot(a)\n",
    "\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2, 2.4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta * a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can see that we've optimally estimated b using a.\n",
    "\n",
    "### Projections in $R^N$\n",
    "Now, how do we manage this using higher dimensions, we need to turn a into a matrix A and b into a vector with the same dimension as the number of rows in A. \n",
    "\n",
    "$$A^T(b-Ax) = 0$$\n",
    "\n",
    "$$A^TAx = A^Tb$$\n",
    "\n",
    "To find x we need to take the inverse of $A^TA$, this is possible because it is always a full rank square matrix.\n",
    "\n",
    "$$x = (A^TA)^{-1}A^Tb$$\n",
    "\n",
    "Thus, we have found our ideal weights in x in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21, 72],\n",
       "       [25, 82],\n",
       "       [73, 19]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randint(0, 100, size=(3, 2))\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of X as variables that can be summed together by some weights to give us our optimal estimates of some vector that does not lie in the column space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77],\n",
       "       [ 1],\n",
       "       [39]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.randint(0, 100, size=(3, 1))\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43665522],\n",
       "       [0.34281468]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.8524166 ],\n",
       "       [39.02718428],\n",
       "       [38.3893102 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X.dot(weights)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonalisation\n",
    "\n",
    "One application of projections in this way is by decomposing a vector into its parallel components.\n",
    "\n",
    "If we cant to decompose vector W relative to V, then we can think of the total vector W as being parallel and perpendicular components of v.\n",
    "\n",
    "$$w = w_{\\parallel v} + w_{\\perp v}$$\n",
    "\n",
    "The component that is parrallel to v will simply be the projection of w onto v:\n",
    "\n",
    "$$w_{\\parallel v} = \\frac{w^Tv}{v^Tv}v$$\n",
    "\n",
    "The vector that is perpendicular to the vector we just calculated can be expressed as the difference between that vector and the original one.\n",
    "\n",
    "$$w_{\\perp v} = w - w_{\\parallel v}$$\n",
    "\n",
    "We can see an example of this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4, 0]), array([2, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([2, 3])\n",
    "v = np.array([4, 0])\n",
    "\n",
    "v, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_para = w.dot(v)/v.dot(v) * v\n",
    "\n",
    "w_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_perp = w - w_para\n",
    "\n",
    "w_perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform the dot product between the two components then we get 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_para.dot(w_perp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
